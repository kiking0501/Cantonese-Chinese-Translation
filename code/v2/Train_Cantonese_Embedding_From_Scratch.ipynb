{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Cantnonese Embedding using Wikipedia data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Wikipedia Dump (zh-yue)\n",
    "- The Wikipedia database dump ver. 20200301: https://dumps.wikimedia.org/zh_yuewiki/20200301/\n",
    "- The dump we are using here is zh_yuewiki-20200301-pages-articles-multistream.xml.bz2 (57.8MB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-21 00:01:57--  https://dumps.wikimedia.org/zh_yuewiki/20200301/zh_yuewiki-20200301-pages-articles-multistream.xml.bz2\n",
      "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.7, 2620:0:861:1:208:80:154:7\n",
      "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.7|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 60625083 (58M) [application/octet-stream]\n",
      "Saving to: ‘zh_yuewiki-20200301-pages-articles-multistream.xml.bz2.1’\n",
      "\n",
      "zh_yuewiki-20200301 100%[===================>]  57.82M  2.93MB/s    in 23s     \n",
      "\n",
      "2020-03-21 00:02:20 (2.50 MB/s) - ‘zh_yuewiki-20200301-pages-articles-multistream.xml.bz2.1’ saved [60625083/60625083]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DUMP_FILE = \"zh_yuewiki-20200301-pages-articles-multistream.xml.bz2\"\n",
    "!wget https://dumps.wikimedia.org/zh_yuewiki/20200301/$DUMP_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 2. Download <a href=\"https://github.com/attardi/wikiextractor\" target=\"_blank\">WikiExtractor</a>\n",
    "\n",
    "- ```WikiExtractor.py``` is a script that extracts and cleans text from a Wikipedia database dump.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-20 23:58:03--  https://github.com/attardi/wikiextractor/archive/master.zip\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://codeload.github.com/attardi/wikiextractor/zip/master [following]\n",
      "--2020-03-20 23:58:03--  https://codeload.github.com/attardi/wikiextractor/zip/master\n",
      "Resolving codeload.github.com (codeload.github.com)... 140.82.113.10\n",
      "Connecting to codeload.github.com (codeload.github.com)|140.82.113.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/zip]\n",
      "Saving to: ‘master.zip.1’\n",
      "\n",
      "master.zip.1            [ <=>                ] 249.29K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2020-03-20 23:58:04 (1.70 MB/s) - ‘master.zip.1’ saved [255270]\n",
      "\n",
      "Archive:  master.zip\n",
      "16186e290d9eb0eb3a3784c6c0635a9ed7e855c3\n",
      "   creating: wikiextractor-master/\n",
      "  inflating: wikiextractor-master/.gitignore  \n",
      "  inflating: wikiextractor-master/README.md  \n",
      "  inflating: wikiextractor-master/WikiExtractor.py  \n",
      "  inflating: wikiextractor-master/categories.filter  \n",
      "  inflating: wikiextractor-master/cirrus-extract.py  \n",
      "  inflating: wikiextractor-master/extract.sh  \n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/attardi/wikiextractor/archive/master.zip\n",
    "!unzip master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 3. Extract wiki articles from the dump <br>\n",
    "``` WikiExtractor.py -o <OUTPUT directory> --json -b 500K <path-to-your-dump-file> ```\n",
    "<br> <br>\n",
    "- `-o <OUTPUT directory>` specifies the output directory\n",
    "- `--json` writes output in json format instead of the default one\n",
    "- `-b 500K` specifies the maximum bytes per output file (default 1M)\n",
    "- `<path-to-your-dump-file>` is the path of the .xml.bz2 file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./wiki_extracts\"\n",
    "\n",
    "!wikiextractor-master/WikiExtractor.py -o $OUTPUT_DIR --json -b 500K $DUMP_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example Log Messages when finished:\n",
    "\n",
    "*INFO: Finished 7-process extraction of 77685 articles in 41.1s (1888.5 art/s) <br>\n",
    "INFO: total of page: 114455, total of articl page: 77685; total of used articl page: 77685*\n",
    "\n",
    "- Example extracted file (\"wiki_00\"):\n",
    "\n",
    "``` {\"url\": \"https://zh-yue.wikipedia.org/wiki?curid=1\", \"text\": \"頭版/2013\\n\", \"id\": \"1\", \"title\": \"頭版/2013\"}\n",
    "{\"url\": \"https://zh-yue.wikipedia.org/wiki?curid=2\", \"text\": \"香港\\n\\n香港（，），係華南一城，亦係一埠，譽為國際大都會。...... 常見嘅街頭小食有雞蛋仔、蛋撻、咖喱魚蛋、燒賣、格仔餅等等，嘢飲就有絲襪奶茶，珍珠奶茶等等。\\n\", \"id\": \"2\", \"title\": \"香港\"}\n",
    "...... ```\n",
    "\n",
    "- Each line in the file is an article stored in the json format. \n",
    "- An extracted file will contain several articles (json), depends on the specified maximum bytes per output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Printing the first line of an example extracted file...\n",
      "\n",
      "{\"text\": \"頭版/2013\\n\", \"id\": \"1\", \"url\": \"https://zh-yue.wikipedia.org/wiki?curid=1\", \"title\": \"頭版/2013\"}\r\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Printing the first line of an example extracted file...\\n\")\n",
    "\n",
    "OUTPUT_SUB_DIR = OUTPUT_DIR + \"/AA\"\n",
    "!head -n 1 $OUTPUT_SUB_DIR/wiki_00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 4. Prepare the Training Corpus\n",
    "\n",
    "- Gensim's word2vec expects a sequence of sentences as its input. Each sentence should be a list of tokens.\n",
    "\n",
    "- To prepare the training corpus, we need to cut the article texts into sentences, followed by a tokenization.\n",
    "\n",
    "- *Note: Jieba tokenization can be customized*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Printing first few sentences:\n",
      "0 | 頭版 / 2013 。\n",
      "1 | 香港 。\n",
      "2 | 香港 （ ， ） ， 係 華南 一城 ， 亦 係 一埠 ， 譽為 國際 大都 會 。\n",
      "3 | 香港 建 於 1841 年 ， 乃 百年 之 城 ， 曾經 係 英國 人 嘅 殖民地 ， 1997 年 開始 由 中華 人民 共和 國 接管 ， 成為 特別 行政 區 。\n",
      "4 | 按 《 中英 聯合 聲明 》 同 《 香港基本法 》 ， 香港 係 高度 自治 嘅 地方 ， 除 外交 同防務 ， 全面 自治 ， 有 參 與 國際 組織 權利 ， 有別 於 中國內 地體制 ， 所以 官方 稱 之 爲 一國 兩制 。\n",
      "5 | 香港 三面 環海 ， 東同南 係 南 中國海 ， 西 珠江口 係 零丁洋 ， 東北 係 大鵬灣 ， 北面 同中 國大陸 隔 住條 深圳河 。\n",
      "6 | 香港 有 唔 少島 ， 散 佈 東南西 。\n",
      "7 | 因為 瀕海 ， 水路 發達 ， 由 太平洋 到 印度洋 ， 各國 洋船 必經 之路 。\n",
      "8 | 空運 亦 係 南洋 同中 國 嘅 樞 紐 ， 全球 每日 都 有 唔 少 飛機 ， 途經 香港 。\n",
      "9 | 香港 同紐 約 、 倫敦合稱 「 紐倫港 」 。\n",
      "10 | 香港 有 幾個 別名 ， 好似 香江 （ ） 、 東方 之珠 （ ） 噉 。\n",
      "11 | 廣東話 裏 面 ， 香江 係 香港 訛音 ， 亦 有 指維 多利 亞港 似 江水 ， 橫 臥 喺 香港 島同 九龍 半島 之間 。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import jieba\n",
    "\n",
    "def read_wiki(file_name):\n",
    "    ''' read json object from an extracted wiki file '''\n",
    "    data = []\n",
    "    with open(os.path.join(OUTPUT_SUB_DIR, file_name), 'r') as f:\n",
    "        for json_obj in f:\n",
    "            data.append(json.loads(json_obj))\n",
    "    return data\n",
    "\n",
    "def read_wiki_sentences(file_name, verbose=True):\n",
    "    ''' Very simple way to cut the wiki article into sentences,\n",
    "        followed by jieba tokenization '''\n",
    "    json_list = read_wiki(file_name)\n",
    "    for json_obj in json_list:\n",
    "        for line in json_obj['text'].split('\\n'):\n",
    "            for l in line.split('。'):\n",
    "                if l:\n",
    "                    yield list(jieba.cut(l, cut_all=False)) + ['。']\n",
    "\n",
    "print(\"=== Printing first few sentences:\")\n",
    "for ind, sentence in enumerate(read_wiki_sentences(\"wiki_00\")):\n",
    "    print(ind, \"|\", \" \".join(sentence))\n",
    "    if ind > 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 5. Train the word embedding using Gensim's word2vec\n",
    "\n",
    "- Modified from <a href=\"https://rare-technologies.com/word2vec-tutorial/\"> this tutorial </a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "\n",
    "- if set `TRAIN_ALL = True`, will use all extracted files for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"wiki_canto\"\n",
    "EMBEDDING_SIZE = 300\n",
    "MIN_COUNT = 5\n",
    "\n",
    "\n",
    "TRAIN_ALL = False\n",
    "\n",
    "if TRAIN_ALL:\n",
    "    FILES = []\n",
    "    for (_, _, filenames) in os.walk(OUTPUT_SUB_DIR):\n",
    "        FILES.extend(filenames)\n",
    "    ITER = 50\n",
    "    \n",
    "else:\n",
    "    FILES = [\"wiki_00\"]\n",
    "    ITER = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-21 00:30:29,414 : INFO : collecting all words and their counts\n",
      "2020-03-21 00:30:29,422 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-03-21 00:30:30,871 : INFO : collected 18627 word types from a corpus of 113134 raw words and 4224 sentences\n",
      "2020-03-21 00:30:30,872 : INFO : Loading a fresh vocabulary\n",
      "2020-03-21 00:30:30,886 : INFO : effective_min_count=5 retains 2701 unique words (14% of original 18627, drops 15926)\n",
      "2020-03-21 00:30:30,888 : INFO : effective_min_count=5 leaves 90666 word corpus (80% of original 113134, drops 22468)\n",
      "2020-03-21 00:30:30,901 : INFO : deleting the raw counts dictionary of 18627 items\n",
      "2020-03-21 00:30:30,903 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2020-03-21 00:30:30,904 : INFO : downsampling leaves estimated 66665 word corpus (73.5% of prior 90666)\n",
      "2020-03-21 00:30:30,915 : INFO : estimated required memory for 2701 words and 300 dimensions: 7832900 bytes\n",
      "2020-03-21 00:30:30,916 : INFO : resetting layer weights\n",
      "2020-03-21 00:30:30,963 : INFO : training model with 3 workers on 2701 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-03-21 00:30:32,033 : INFO : EPOCH 1 - PROGRESS: at 63.64% examples, 37169 words/s, in_qsize 0, out_qsize 0\n",
      "2020-03-21 00:30:32,605 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-03-21 00:30:32,606 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-03-21 00:30:32,612 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-03-21 00:30:32,613 : INFO : EPOCH - 1 : training on 113134 raw words (66575 effective words) took 1.6s, 40403 effective words/s\n",
      "2020-03-21 00:30:32,613 : INFO : training on a 113134 raw words (66575 effective words) took 1.6s, 40360 effective words/s\n",
      "2020-03-21 00:30:32,614 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-03-21 00:30:32,616 : INFO : saving Word2Vec object under wiki_canto.model, separately None\n",
      "2020-03-21 00:30:32,617 : INFO : not storing attribute vectors_norm\n",
      "2020-03-21 00:30:32,618 : INFO : not storing attribute cum_table\n",
      "2020-03-21 00:30:32,763 : INFO : saved wiki_canto.model\n",
      "2020-03-21 00:30:32,764 : INFO : storing 2701x300 projection weights into wiki_canto.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.05336472193400065 mins.\n",
      "wiki_canto saved.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "class CantoSentences(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file_name in FILES:\n",
    "            for sentence in read_wiki_sentences(file_name):\n",
    "                yield sentence\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "corpus = CantoSentences()\n",
    "model = Word2Vec(corpus, size=EMBEDDING_SIZE, min_count=MIN_COUNT, iter=ITER)\n",
    "\n",
    "print(\"Training time: %s mins.\" % ((time.time() - st) / 60))\n",
    "\n",
    "model.save(\"%s.model\" % MODEL_NAME)\n",
    "model.wv.save_word2vec_format('%s.bin' % MODEL_NAME, binary=True)\n",
    "print(\"%s saved.\" % MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('以', 0.9989677667617798),\n",
       " ('後', 0.9989623427391052),\n",
       " ('有', 0.9989590048789978),\n",
       " ('做', 0.9989559054374695),\n",
       " ('；', 0.9989538192749023),\n",
       " ('嚟', 0.9989529848098755),\n",
       " ('綫', 0.9989511370658875),\n",
       " ('，', 0.9989491105079651),\n",
       " ('而', 0.9989476799964905),\n",
       " ('唔', 0.9989449977874756)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"廣東話\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "\n",
    "## 6. Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-21 00:37:06,783 : INFO : loading Word2Vec object from ../../data/embedding/cantonese/custom_wiki.model\n",
      "2020-03-21 00:37:07,027 : INFO : loading wv recursively from ../../data/embedding/cantonese/custom_wiki.model.wv.* with mmap=None\n",
      "2020-03-21 00:37:07,029 : INFO : loading vectors from ../../data/embedding/cantonese/custom_wiki.model.wv.vectors.npy with mmap=None\n",
      "2020-03-21 00:37:07,077 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-03-21 00:37:07,077 : INFO : loading trainables recursively from ../../data/embedding/cantonese/custom_wiki.model.trainables.* with mmap=None\n",
      "2020-03-21 00:37:07,078 : INFO : loading syn1neg from ../../data/embedding/cantonese/custom_wiki.model.trainables.syn1neg.npy with mmap=None\n",
      "2020-03-21 00:37:07,120 : INFO : loading vocabulary recursively from ../../data/embedding/cantonese/custom_wiki.model.vocabulary.* with mmap=None\n",
      "2020-03-21 00:37:07,121 : INFO : setting ignored attribute cum_table to None\n",
      "2020-03-21 00:37:07,121 : INFO : loaded ../../data/embedding/cantonese/custom_wiki.model\n"
     ]
    }
   ],
   "source": [
    "SAVED_MODEL = \"../../data/embedding/cantonese/custom_wiki.model\"\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "saved_model = Word2Vec.load(SAVED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-21 00:37:34,563 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('粵語', 0.569934070110321),\n",
       " ('廣州話', 0.4621201455593109),\n",
       " ('普通話', 0.4244949221611023),\n",
       " ('閩南話', 0.4133637547492981),\n",
       " ('客家話', 0.4086306095123291),\n",
       " ('口語', 0.40600326657295227),\n",
       " ('官話', 0.4033676087856293),\n",
       " ('俗語', 0.398201584815979),\n",
       " ('中文', 0.3963436484336853),\n",
       " ('文言', 0.3910912871360779)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model.wv.similar_by_word(\"廣東話\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
