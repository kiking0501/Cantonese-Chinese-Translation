{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.7"
    },
    "colab": {
      "name": "COLAB_Train_Cantonese_BERT.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiking0501/Cantonese-Chinese-Translation/blob/master/code/v2/COLAB_Train_Cantonese_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDSkPs5FcNOa",
        "colab_type": "text"
      },
      "source": [
        "# **An Example to Train Cantonese-BERT**\n",
        "\n",
        "- Below is designed to be run in Colab Jupyter Environment with a GCS bucket. If you are unsure about this, check <a href=\"https://medium.com/fenwicks/tutorial-0-setting-up-google-colab-tpu-runtime-and-cloud-storage-b88d34aa9dcb\" target=\"_blank\">here</a>.\n",
        "\n",
        "- To resume training after a disconnection, run only cells with <!>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WsKOLeNIQpy",
        "colab_type": "text"
      },
      "source": [
        "## **<!>Specify Tensorflow 1.X version**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0pA-gQG9Hl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC5rjpfId63f",
        "colab_type": "text"
      },
      "source": [
        "## **<!>Setup GCS bucket name**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE2YN_zQeP2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUCKET_NAME = \"bert_cantonese\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0FqRDukIY41",
        "colab_type": "text"
      },
      "source": [
        "## **<!>Authorize to GCS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import nltk\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import auth, drive\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "auth.authenticate_user()\n",
        "  \n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "# create formatter and add it to the handlers\n",
        "formatter = logging.Formatter('%(asctime)s :  %(message)s')\n",
        "sh = logging.StreamHandler()\n",
        "sh.setLevel(logging.INFO)\n",
        "sh.setFormatter(formatter)\n",
        "log.handlers = [sh]\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  USE_TPU = True\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "  USE_TPU = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I554Ij67IHER",
        "colab_type": "text"
      },
      "source": [
        "## **Download Bert**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNaqcWom6tYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/google-research/bert\n",
        "\n",
        "sys.path.append(\"bert\")\n",
        "from bert import modeling, optimization, tokenization\n",
        "from bert.run_pretraining import input_fn_builder, model_fn_builder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umY9iZotfZ6K",
        "colab_type": "text"
      },
      "source": [
        "### **Or, if you have a customized BERT folder in GCS bucket**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSGq12okfZc3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil -m cp -r $BUCKET_PATH/code/v2/bert ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAjHvqKigiVq",
        "colab_type": "text"
      },
      "source": [
        "## **Download Wikipedia Data, WikiExtractor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIbVjaS86tYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DUMP_FILE = zh_yuewiki-20200301-pages-articles-multistream.xml.bz2\n",
        "!wget https://dumps.wikimedia.org/zh_yuewiki/20200301/$DUMP_FILE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mADPr9SvAvbe",
        "colab_type": "code",
        "outputId": "2bc2e28b-b816-44ca-a4aa-e580bdbad714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "!wget https://github.com/attardi/wikiextractor/archive/master.zip\n",
        "!unzip master.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-20 22:15:07--  https://github.com/attardi/wikiextractor/archive/master.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/attardi/wikiextractor/zip/master [following]\n",
            "--2020-03-20 22:15:07--  https://codeload.github.com/attardi/wikiextractor/zip/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.112.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.112.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [ <=>                ] 249.29K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-03-20 22:15:08 (2.37 MB/s) - ‘master.zip’ saved [255270]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BA5VUPrhYds",
        "colab_type": "text"
      },
      "source": [
        "## **Extract Wiki Files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gw79bY4A-U7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python wikiextractor-master/WikiExtractor.py -o . --json -b 500k zh_yuewiki-20200301-pages-articles-multistream.xml.bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMSnZt4FBKjZ",
        "colab_type": "code",
        "outputId": "262dd9dc-9cfa-4f63-f66c-82f82caf707d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "!ls ./AA"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wiki_00  wiki_11  wiki_22  wiki_33  wiki_44  wiki_55  wiki_66  wiki_77\twiki_88\n",
            "wiki_01  wiki_12  wiki_23  wiki_34  wiki_45  wiki_56  wiki_67  wiki_78\twiki_89\n",
            "wiki_02  wiki_13  wiki_24  wiki_35  wiki_46  wiki_57  wiki_68  wiki_79\twiki_90\n",
            "wiki_03  wiki_14  wiki_25  wiki_36  wiki_47  wiki_58  wiki_69  wiki_80\twiki_91\n",
            "wiki_04  wiki_15  wiki_26  wiki_37  wiki_48  wiki_59  wiki_70  wiki_81\twiki_92\n",
            "wiki_05  wiki_16  wiki_27  wiki_38  wiki_49  wiki_60  wiki_71  wiki_82\twiki_93\n",
            "wiki_06  wiki_17  wiki_28  wiki_39  wiki_50  wiki_61  wiki_72  wiki_83\twiki_94\n",
            "wiki_07  wiki_18  wiki_29  wiki_40  wiki_51  wiki_62  wiki_73  wiki_84\twiki_95\n",
            "wiki_08  wiki_19  wiki_30  wiki_41  wiki_52  wiki_63  wiki_74  wiki_85\twiki_96\n",
            "wiki_09  wiki_20  wiki_31  wiki_42  wiki_53  wiki_64  wiki_75  wiki_86\n",
            "wiki_10  wiki_21  wiki_32  wiki_43  wiki_54  wiki_65  wiki_76  wiki_87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blKAr5jcEbIY",
        "colab_type": "code",
        "outputId": "426cb5fd-5f6d-477d-b673-21e30c21127e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "!mv ./AA ./json\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json    sample_data\n",
            "bert\t    wikiextractor-master\n",
            "json\t    zh_yuewiki-20200301-pages-articles-multistream.xml.bz2\n",
            "master.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JfYq5mUio9u",
        "colab_type": "text"
      },
      "source": [
        "## **Save Clean Wiki Files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU-otlIEFCfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import re\n",
        "import jieba\n",
        "\n",
        "DATA_PATH = \".\"\n",
        "WIKI_PATH = DATA_PATH\n",
        "WIKI_ORI_PATH = os.path.join(DATA_PATH, \"json\")\n",
        "\n",
        "def read_wiki(file_name):\n",
        "    data = []\n",
        "    with open(os.path.join(WIKI_ORI_PATH, file_name), 'r') as f:\n",
        "        for json_obj in f:\n",
        "            data.append(json.loads(json_obj))\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_clean_wiki(file_name, verbose=True):\n",
        "    json_list = read_wiki(file_name)\n",
        "    for json_obj in json_list:\n",
        "        output_path = os.path.join(\".\", \"clean\", \"%s_%s\" % (json_obj['id'], json_obj['title'].replace('/', '-').strip()))\n",
        "        with open(output_path, \"w\") as f:\n",
        "            for line in json_obj['text'].split('\\n'):\n",
        "                content = re.findall('\\（.*?\\）', line)\n",
        "\n",
        "                for l in line.split('。'):\n",
        "                    if l:\n",
        "                        f.write(' '.join(jieba.cut(l, cut_all=False)) + ' 。\\n')\n",
        "        print(\"%s saved.\" % output_path)\n",
        "\n",
        "\n",
        "def read_clean_wiki(file_code):\n",
        "    with open(os.path.join(WIKI_PATH, \"clean\", file_code)) as f:\n",
        "        return [l.strip().split(' ') for l in f]\n",
        "\n",
        "\n",
        "def save_clean_wikipedia(output_file=\"wiki_yue_overview.csv\", verbose=True):\n",
        "    load_jieba()\n",
        "    for (_, _, filenames) in sorted(os.walk(WIKI_ORI_PATH)):\n",
        "        for file_name in sorted(filenames):\n",
        "            save_clean_wiki(file_name)\n",
        "    total = 0\n",
        "    with open(os.path.join(WIKI_PATH, output_file), \"w\") as f:\n",
        "        file_codes = []\n",
        "        for (_, _, filenames) in os.walk(os.path.join(\".\", \"clean\")):\n",
        "            file_codes.extend(filenames)\n",
        "        for ind, code in enumerate(sorted(file_codes)):\n",
        "            f.write('%s,%s\\n' % (ind, code))\n",
        "        total += len(file_codes)\n",
        "    if verbose:\n",
        "        print(\"Total: %d. %s saved.\" % (total, output_file))\n",
        "\n",
        "\n",
        "def read_clean_wikipedia(overview_csv=\"wiki_yue_overview.csv\"):\n",
        "    with open(os.path.join(WIKI_PATH, overview_csv)) as f:\n",
        "        for line in f.readlines():\n",
        "            # print(line)\n",
        "            _, file_code = line.partition(',')[0], line.partition(',')[2].strip()\n",
        "            if file_code.startswith('wiki'):\n",
        "                continue\n",
        "            # print(\"Reading %s..\" % file_code)\n",
        "            for sen in read_clean_wiki(file_code):\n",
        "                yield sen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQQI0F6giuB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir clean\n",
        "save_clean_wikipedia()\n",
        "\n",
        "output_path = \"wiki_dataset.txt\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    for ind, sen in enumerate(read_clean_wikipedia()):\n",
        "        f.write(\"%s\\n\" % \" \".join(sen))\n",
        "    print(\"%s saved.\" % f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSLg61KOWkc7",
        "colab_type": "text"
      },
      "source": [
        "## **Create Vocab Files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klUVyx8MWxES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp $GCS/data/embedding/cantonese/custom_wiki.bin ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny-AGrlP6tZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "model = KeyedVectors.load_word2vec_format(\"./custom_wiki.bin\", binary=True)\n",
        "print(model.index2word[:1000])\n",
        "\n",
        "bert_vocab = model.index2word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM3Hbay26tZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seNfGT-T6tZv",
        "colab_type": "code",
        "outputId": "5eb86faf-363e-4369-998c-0080f715127b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(VOC_SIZE - len(bert_vocab))]\n",
        "print(len(bert_vocab))\n",
        "VOC_SIZE = len(bert_vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvnhPgXE6tZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68B08VEB6taH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!head -n 50 $VOC_FNAME"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gactQSD26tad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testcase = \"香港士巴拿係一種架生，作用係方便上緊或者扭鬆正方形同六角形嘅螺絲頭同螺絲帽，手柄畀人揸住用力。\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_Un3vxTy6tap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "bert_tokenizer.tokenize(testcase)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA4tAWeQbmb7",
        "colab_type": "text"
      },
      "source": [
        "## **Create Local Shard, Generate PreTraining Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-6ZAeJY6tax",
        "colab_type": "code",
        "outputId": "e9cf0b7f-f1d0-4180-8371-9cf02bad0427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "!mkdir ./shards\n",
        "!split -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_\n",
        "!ls ./shards/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shard_0000  shard_0001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bip2_BO6tbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "DO_LOWER_CASE = True #@param {type:\"boolean\"}\n",
        "PROCESSES = 8 #@param {type:\"integer\"}\n",
        "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r_45IS76tbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XARGS_CMD = (\"ls ./shards/ | \"\n",
        "             \"xargs -n 1 -P {} -I{} \"\n",
        "             \"python3 bert/create_pretraining_data.py \"\n",
        "             \"--input_file=./shards/{} \"\n",
        "             \"--output_file={}/{}.tfrecord \"\n",
        "             \"--vocab_file={} \"\n",
        "             \"--do_lower_case={} \"\n",
        "             \"--max_predictions_per_seq={} \"\n",
        "             \"--max_seq_length={} \"\n",
        "             \"--masked_lm_prob={} \"\n",
        "             \"--random_seed=34 \"\n",
        "             \"--dupe_factor=5\")\n",
        "\n",
        "XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', \n",
        "                             VOC_FNAME, DO_LOWER_CASE, \n",
        "                             MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcwdZ-AV6tbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.gfile.MkDir(PRETRAINING_DIR)\n",
        "!$XARGS_CMD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXGEw7vbb2X-",
        "colab_type": "text"
      },
      "source": [
        "### **Or, if you already have PreTraining Data from GCS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWBO-9qEb_LJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil -m cp -r $GCS/code/v2/$PRETRAINING_DIR ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIbrgZlacRxb",
        "colab_type": "text"
      },
      "source": [
        "## **Create Trained Model Directory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGNlx1aB6tba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_DIR = \"bert_model\" #@param {type:\"string\"}\n",
        "tf.gfile.MkDir(MODEL_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzWpy1ZZ6tbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use this for BERT-base\n",
        "\n",
        "bert_base_config = {\n",
        "  \"attention_probs_dropout_prob\": 0.1, \n",
        "  \"directionality\": \"bidi\", \n",
        "  \"hidden_act\": \"gelu\", \n",
        "  \"hidden_dropout_prob\": 0.1, \n",
        "  \"hidden_size\": 768, \n",
        "  \"initializer_range\": 0.02, \n",
        "  \"intermediate_size\": 3072, \n",
        "  \"max_position_embeddings\": 512, \n",
        "  \"num_attention_heads\": 12, \n",
        "  \"num_hidden_layers\": 12, \n",
        "  \"pooler_fc_size\": 768, \n",
        "  \"pooler_num_attention_heads\": 12, \n",
        "  \"pooler_num_fc_layers\": 3, \n",
        "  \"pooler_size_per_head\": 128, \n",
        "  \"pooler_type\": \"first_token_transform\", \n",
        "  \"type_vocab_size\": 2, \n",
        "  \"vocab_size\": VOC_SIZE\n",
        "}\n",
        "\n",
        "with open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
        "  json.dump(bert_base_config, fo, indent=2)\n",
        "  \n",
        "with open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx4eCuxulspF",
        "colab_type": "text"
      },
      "source": [
        "### **Backup Directories to GCS Bucket: PreTraining Data and Trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qhs_TmY6tbp",
        "colab_type": "code",
        "outputId": "9e066689-d0bd-4c13-bb8b-65c57c670379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "!gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://bert_model/vocab.txt [Content-Type=text/plain]...\n",
            "Copying file://bert_model/bert_config.json [Content-Type=application/json]...\n",
            "Copying file://pretraining_data/shard_0001.tfrecord [Content-Type=application/octet-stream]...\n",
            "Copying file://pretraining_data/shard_0000.tfrecord [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "|\n",
            "Operation completed over 4 objects/530.5 MiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd9bShbic2Bq",
        "colab_type": "text"
      },
      "source": [
        "## **<!> Set Training Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0wJL1vv6tbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "BUCKET_NAME = \"bert_cantonese\" #@param {type:\"string\"}\n",
        "MODEL_DIR = \"bert_model\" #@param {type:\"string\"}\n",
        "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}\n",
        "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "# Input data pipeline config\n",
        "TRAIN_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "\n",
        "# Training procedure config\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LEARNING_RATE = 2e-5\n",
        "TRAIN_STEPS = 1000000 #@param {type:\"integer\"}\n",
        "SAVE_CHECKPOINTS_STEPS = 2500 #@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "if BUCKET_NAME:\n",
        "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "else:\n",
        "  BUCKET_PATH = \".\"\n",
        "\n",
        "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
        "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
        "\n",
        "VOCAB_FILE = os.path.join(BERT_GCS_DIR, VOC_FNAME)\n",
        "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")\n",
        "\n",
        "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "\n",
        "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
        "\n",
        "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
        "log.info(\"Using {} data shards\".format(len(input_files)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKh7mQ9u6tb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=TRAIN_STEPS,\n",
        "      num_warmup_steps=10,\n",
        "      use_tpu=USE_TPU,\n",
        "      use_one_hot_embeddings=True)\n",
        "\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=BERT_GCS_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=USE_TPU,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)\n",
        "  \n",
        "train_input_fn = input_fn_builder(\n",
        "        input_files=input_files,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "        is_training=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ped7NlkB6tcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXXEUPRy6tcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}